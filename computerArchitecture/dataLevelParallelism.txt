Data-Level Parallelism
Introduction
<1 min

In computer architecture, a pipeline is associated with instruction-level parallelism in which multiple instructions can be processed simultaneously. 
As the computing landscape moves toward data-intensive tasks, there is a need for higher throughput of data processing. This is known as data-level parallelism (DLP).

The following lesson will explore data-level parallelism by:

    Defining three DLP approaches
    Connecting how DLP approaches influence each other
    Exploring the hardware implementations of each application

While things like big data and advanced machine learning algorithms are still relatively new, data-level parallelism has been around for a while. 
Its evolution has been influenced by research as well as industrial, corporate and consumer needs. The data-parallelism landscape is currently 
changing and it is important to know how it began and where it is going. Have fun and enjoy the lesson!



SIMD
1 min

Imagine you’re a dog walker who has four clients who need their dogs walked in the afternoon. You start your first day walking the first dog. 
You return with the first and start walking the second. Then, you return with the second and get the third, and so on. After three hours, you 
have finished walking all four dogs and return home.

You repeat this daily until one day you have an idea. What if you walk all the dogs together? You perform the same task for each dog so walking 
them together will save you some time.

This is the idea behind Single Instruction, Multiple Data (SIMD). If the same function is to be applied to each data element, one instruction can 
be called to act on all the elements. SIMD is one approach to data-level parallelism and is almost as old as the modern computer era.

The different applications of SIMD are:

    Vector Processing
    SIMD Extensions
    Graphical Processing Units (GPUs)




Vector Processing
3 min

Vector processing was conceived in the 1960s and is one of the earliest applications of SIMD computing. Instead of processing one value at a time using a single 
instruction, researchers were looking for ways to use a single instruction to process multiple related values, known as vectors. The Cray-1 supercomputer is said 
to be one of the first supercomputers to successfully implement vector processing.

The following are mechanisms that leverage the properties of vector processing to benefit performance:


Less Instruction Overhead

With one instruction per vector, there will be far fewer fetch and decode stages per data. After the initial overhead of a single instruction fetch and decode, multiple data elements can go through the pipeline using the rest of the stages.
Memory Access Bandwidth

It is not only instructions that can be pipelined, but also memory accesses. When an instruction begins, a certain amount of data is retrieved from memory. When data is moved to the next stage in the pipeline the next set of data can be retrieved from memory. This overlapping of memory accesses reduces the overhead involved in accessing such large amounts of data from memory.
Pipelining

A vector
Preview: Docs Loading link description
processor
utilizes the Fetch, Decode, Execute, Memory Access and Write Back instruction process and therefore can leverage pipelining to increase the throughput of data.



Vector Architecture
3 min

So how is a vector architecture different from a scalar one?
Vector Registers

To operate on large amounts of data, the
Preview: Docs Loading link description
CPU
will need somewhere to put it. Vector registers are just like regular single element registers except they can hold multiple elements of data. The Cray-1 worked on 
64-bit data and each vector register could hold up to 64 elements of data. This is a register that is: 64 x 64 = 4096 bits long!
Internal Looping

A code loop is how a scalar
Preview: Docs A processor is the electric circuitry found in computer hardware which is responsible for executing instruction sets derived from programs that have 
been converted into machine code.
processor
performs the same operation on multiple points of data. While there are mechanisms to simplify coded loops, there will always be multiple instruction fetches and 
decodes per loop. Since a vector architecture commonly performs the same operation on vectors of data, the looping is assumed and can be built into the architecture 
without any extra instruction fetches.
Lanes

Vector architectures can also use multiple lanes to process data elements simultaneously.
Each lane can hold all the functional units a scalar processor has such as arithmetic, floating-point, and logic units. This comes at the cost of a more complex 
architecture, but they can help speed up processing drastically.

Vector processors also contain much of the same hardware that scalar processors do. One example is scalar registers to hold immediate values. If a program needs to add 5 to an entire vector register, it makes sense to hold the value 5 in a scalar register and not one element of a vector register.




Data-Level Parallelism
SIMD Extensions
2 min

Processors used for personal and business computers started out as scalar processors where the registers and functional units were meant for single elements of data. 
With the increase in data-intensive multimedia applications,
Preview: Docs Loading link description
processor
manufacturers started looking at ways they could leverage the benefits of vector processing.

The result was a new processor design that maintained the scalar functionality but added components of vector processors. These additions are known as SIMD Extensions.

Every major processor company has some form of SIMD extension. As data-intensive tasks have become more mainstream the need to improve the performance of the extensions grew.

The x86 instruction set architecture introduced SIMD extensions in the late 1990s with Streaming SIMD Extensions (SSE). The architecture added instructions that 
worked on 8 registers labeled xmm0 - xmm7. These registers were 128 bits long and could hold up to four 32-bit floating point values.

Over the years these SIMD Extensions were updated with more instructions and include zmm registers that are up to 512-bits long. These additional capabilities 
have been essential to keeping up with the increasing amount of data used by commonly used applications.


GPUs
2 min

As long as there was a need for visual output on a computer, Graphical Processing Units (GPUs) existed. They provide the specific function of handling all the information to be output to the user as graphics.

Today’s graphical applications are demanding. GPUs are used for a variety of applications including video games and video processing applications. GPUs also perform well with many non-graphical applications like digital signal processing, machine learning applications, and
Preview: Docs Loading link description
cryptocurrency
.

GPUs are separate architectures from CPUs and are used to take on more graphic-intensive jobs. Some CPUs come with GPUs included in their chip, while other CPUs require a GPU to be added as an external piece of hardware.

GPUs are SIMD architectures that focus on efficiently processing graphical data. Because of this, they differ from standard scalar and vector CPUs in the following ways:

    Operations on graphical data tend to be simple but at a very large scale so today’s GPUs have simpler functional units but a lot of them.
    GPU clock speeds lag way behind standard CPUs
    While processing branches, in general, is a challenge, GPUs are especially inefficient with applications involving branching.

Because of higher demand applications, GPU companies have evolved their architectures to address the specific nature of their workload: simple operations on a lot of data. To accomplish this the Single Instruction Multiple Thread (SIMT) architecture was created.

SIMT operates on data that needs simple processing. A single operation can be sent to many small processing units called threads. These threads synchronously process data in bulk. As design capabilities get better GPUs can handle more threads and therefore process more data.



Skip to Content
My Home
Data-Level Parallelism Problem Set

Apply your knowledge of data-level parallelism with free-response questions about SIMD, vector processing, SIMD extensions, and GPUs.

Answer the following questions using what you learned about data-level parallelism. These questions are meant to challenge your understanding of the concepts covered in the DLP lesson. Do your best to answer each question and refer back to the content if you need to refresh your knowledge of any aspects of data-level parallelism.
Free response

Instruction-level parallelism and data-level parallelism both aim to achieve higher processing efficiency, but through differing approaches.

Identify at least 1 difference and 1 similarity between ILP and DLP.

Your response

ys
Our answer
Difference

Number of instructions: ILP aims to make instruction processing more efficient but this approach uses more instructions to process the same amount of data than DLP.
Similarity

One of the main approaches used in ILP is pipelining, where the instruction process is broken up into pieces and each step of separate instructions can be overlapped to increase efficiency. Since DLP uses the same instruction process it is also able to leverage pipelining to increase efficiency.
Free response

Based on what you learned about vector architecture, think of 3 real-world examples that have similar attributes. Explain how each example is similar to vector architecture.

Be sure to use elements from the content pertaining to SIMD, vector processing, and vector architecture.

Your response

n
Our answer
Example 1

When prepping ingredients for a meal, you ask family members to help. One might chop onions, one might chop carrots and you might chop celery. The act of applying the process of chopping simultaneously is an example of SIMD, and the vector architecture component, lanes.
Example 2

When doing laundry we put multiple articles of clothing in the washer and dryer. This is equivalent to SIMD and placing a vector of data in a register and applying one instruction to the entire collection. It would be inefficient to wash one article of clothing at a time.
Example 3

When a librarian puts books back on the shelf, instead of sorting through a messy pile of books, the books are placed in a cart stacked one next to the other in a row. This organization makes it easier to take one book after another and identify its location on the shelves. This can be similar to internal looping and how the processor prepares the data to be processed with the same operation over and over again.
Free response

Name 2 components SIMD extensions share with vector architectures. Describe each one’s function and how they help increase data throughput.

Your response

e
Our answer
Vector Registers

Adding vector registers through SIMD extensions, processors can store multiple elements of data to be processed. This allows for data to be retrieved from memory with less overhead and have a single instruction act on multiple elements of data. These vector registers are also accessible by the scalar components and functionality.
Lanes

Having many paths for data to be processed allows for multiple elements of data to be processed at the same time. Each lane can have multiple functional units such as arithmetic, floating-point and logical.
Free response

Data-level parallelism began with vector processing and is now breaking new ground with GPUs using SIMT architectures. Using your own words, write 1-2 sentences that describe the classic vector architecture and the SIMT architecture.

Your response

e
Our answer

Vector architectures allow for the storage of vector data, organizing of the data, and use a single instruction to process the data with the same, sometimes complex, operation.

SIMT architectures pass data through threads along with the desired instruction. The threads use simple operations so the architecture can contain a large number of threads to provide large data throughput.
Back (alt + , )
Next (alt + . )


What Is A Merge Sort?
1 min

Merge sort is a sorting
Preview: Docs Loading link description
algorithm
created by John von Neumann in 1945. Merge sort’s “killer app” was the strategy that breaks the list-to-be-sorted into smaller parts, 
sometimes called a divide-and-conquer algorithm.

In a divide-and-conquer algorithm, the data is continually broken down into smaller elements until sorting them becomes really simple.

Merge sort was the first of many sorts that use this strategy, and is still in use today in many different applications.


How To Merge Sort:
1 min

Merge sorting takes two steps: splitting the data into “runs” or smaller components, and the re-combining those runs into sorted lists (the “merge”).

When splitting the data, we divide the input to our sort in half. We then recursively call the sort on each of those halves, which cuts the halves into quarters. 
This process continues until all of the lists contain only a single element. Then we begin merging.

When merging two single-element lists, we check if the first element is smaller or larger than the other. Then we return the two-element list with the smaller 
element followed by the larger element.



Merging
2 min

When merging larger pre-sorted lists, we build the list similarly to how we did with single-element lists.

Let’s call the two lists left and right. Both left and right are already sorted. We want to combine them (to merge them) into a larger sorted list, 
let’s call it both. To accomplish this we’ll need to iterate through both with two indices, left_index and right_index.

At first left_index and right_index both point to the start of their respective lists. left_index points to the smallest element of left (its first element) 
and right_index points to the smallest element of right.

Compare the elements at left_index and right_index. The smaller of these two elements should be the first element of both because it’s the smallest of 
both! It’s the smallest of the two smallest values.

Let’s say that smallest value was in left. We continue by incrementing left_index to point to the next-smallest value in left. Then we compare the 
2nd smallest value in left against the smallest value of right. Whichever is smaller of these two is now the 2nd smallest value of both.

This process of “look at the two next-smallest elements of each list and add the smaller one to our resulting list” continues on for as long as both 
lists have elements to compare. Once one list is exhausted, say every element from left has been added to the result, then we know that all the elements 
of the other list, right, should go at the end of the resulting list (they’re larger than every element we’ve added so far).


Merge sort performance 

Merge sort was unique for its time in that the best, worst, and average time complexity are all the same: Θ(N*log(N)). This means an almost-sorted list will 
take the same amount of time as a completely out-of-order list. This is acceptable because the worst-case scenario, where a sort could stand to 
take the most time, is as fast as a sorting
Preview: Docs An algorithm is a formal process used to solve a problem. They can be represented in several formats but are usually represented in 
pseudocode in order to communicate the process by which the algorithms solve the problems they were created to tackle.
algorithm
can be.

Some sorts attempt to improve upon the merge sort by first inspecting the input and looking for “runs” that are already pre-sorted. Timsort is one such 
algorithm that attempts to use pre-sorted data in a list to the sorting algorithm’s advantage. If the data is already sorted, Timsort runs in Θ(N) time.

Merge sort also requires space. Each separation requires a temporary
Preview: Docs Loading link description
array
, and so a merge sort would require enough space to save the whole of the input a second time. This means the worst-case space complexity of merge sort is O(N).







Explanation of the Merging algorithm

The time complexity of Merge Sort is O(nlog⁡n)O(nlogn), and the space complexity is O(n)O(n). Let's break down why the algorithm has these complexities.
Time Complexity of Merge Sort: O(n \log n)

Merge Sort is a divide-and-conquer algorithm, which works by recursively dividing the array into two halves, sorting each half, and then merging the sorted halves. 
Here's how this leads to O(nlog⁡n)O(nlogn) time complexity.
Divide-and-Conquer Process:

    Divide the array into two halves.
    Conquer by recursively sorting each half.
    Combine (merge) the two sorted halves.

Step-by-Step Breakdown:

    Dividing the array: Each time we split the array, we divide it into two smaller subarrays. This division continues until we have subarrays of 
    size 1 (which are inherently sorted).
        The process of dividing the array takes logarithmic time, because the array is halved at every step. The depth of the recursion tree is log₂n, 
        where nn is the number of elements in the array. Therefore, the division process requires O(log⁡n)O(logn) recursive calls.

    Merging the subarrays: Once the subarrays are divided into single elements, they need to be merged back into larger sorted arrays.
        Merging two subarrays takes linear time relative to the size of the subarrays. For an array of size nn, merging requires at most O(n) comparisons.

Putting It Together:

    At each level of the recursion tree, we perform O(n) work to merge the subarrays.
    There are log₂n levels of recursion (because the array is repeatedly halved), and at each level, we do O(n) work to merge the subarrays.

Thus, the total time complexity is:
O(nlog⁡n)
O(nlogn)
Visualizing the Recursion Tree:

    At the top level (level 1), the array is split into 2 parts, and merging takes O(n) time.
    At level 2, each of those two parts is split again, so you have 4 parts, but merging still takes O(n) time (because all parts together still have size nn).
    This continues for log₂n levels, and at each level, the total merging time is O(n).

Space Complexity of Merge Sort: O(n)

Merge Sort requires additional space for storing temporary arrays during the merge process. Here's how the space complexity becomes O(n)O(n):
Why Merge Sort Needs Extra Space:

    Auxiliary Space for Merging:
        When merging two sorted subarrays, a temporary array is needed to hold the combined sorted elements before they are copied back into the original array. 
        This requires extra space proportional to the size of the array being sorted.
        For each recursive call, we allocate additional space to merge the two halves. The size of this temporary array is equal to the size of the input array, 
        so at each level, we need O(n) extra space.

    Recursive Call Stack:
        The recursion stack depth is O(log n) because the array is repeatedly halved. However, this logarithmic space is much smaller than the O(n) space 
        required for merging.

Thus, the dominant factor in space complexity is the auxiliary space for merging, which results in an overall space complexity of O(n).
Putting It Together:

    Time complexity: O(nlog⁡n)O(nlogn) because:
        The array is divided recursively (log₂n divisions), and
        Merging two subarrays takes linear time O(n) at each level of the recursion tree.
    Space complexity: O(n)O(n) because:
        Temporary arrays are used during merging, and the total space used is proportional to the input size nn.

Summary:

    Time Complexity: O(nlog⁡n)O(nlogn) due to the recursive division and merging process.
    Space Complexity: O(n)O(n) due to the need for auxiliary space to merge the subarrays.